{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d6d5a1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.01001,
     "end_time": "2024-05-18T10:29:19.495064",
     "exception": false,
     "start_time": "2024-05-18T10:29:19.485054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">  Hands-On LangChain for LLM Applications Development: Chatbots Memory </center>\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "When interacting with language models, such as Chatbots, the absence of memory poses a significant hurdle in creating natural and seamless conversations. Users expect continuity and context retention, which traditional models lack. This limitation becomes particularly evident in applications where ongoing dialogue is crucial for user engagement and satisfaction.\n",
    "\n",
    "LangChain offers robust solutions to address this challenge. Memory, in this context, refers to the ability of the language model to remember previous parts of a conversation and use that information to inform subsequent interactions. By incorporating memory into the model’s architecture, LangChain enables Chatbots and similar applications to maintain a conversational flow that mimics human-like dialogue.\n",
    "\n",
    "LangChain’s memory capabilities extend beyond mere recall of past interactions. It encompasses sophisticated mechanisms for storing, organizing, and retrieving relevant information, ensuring that the Chatbot can respond appropriately based on the context of the conversation. This not only enhances the user experience but also enables the Chatbot to provide more accurate and relevant responses over time.\n",
    "\n",
    "#### <a id=\"top\"></a>\n",
    "# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b>Table of contents</b></div>\n",
    "\n",
    "<div style=\"background-color: rgba(60, 121, 245, 0.03); padding:30px; font-size:15px; font-family: consolas;\">\n",
    "<ul>\n",
    "    <li><a href=\"#1\" target=\"_self\" rel=\" noreferrer nofollow\">1. Setting Up Working Environment & Getting Starting </a> </li>\n",
    "    <li><a href=\"#2\" target=\"_self\" rel=\" noreferrer nofollow\">2. Conversation Buffer Memory </a></li>\n",
    "    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">3. Conversation Buffer Window Memory </a></li> \n",
    "    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">4. Conversation Token Buffer Memory </a></li> \n",
    "    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">5. Conversation Summary Memory </a></li> \n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cbe6aa",
   "metadata": {
    "papermill": {
     "duration": 0.009759,
     "end_time": "2024-05-18T10:29:19.514730",
     "exception": false,
     "start_time": "2024-05-18T10:29:19.504971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 1. Setting Up Working Environment & Getting Started </b></div>\n",
    "\n",
    "To get started we are going to import OS, import OpenAI, and load my OpenAI secret key. If you’re running this locally, and you don’t have OpenAI installed yet, you might need to run pip to install OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b71cc8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:29:19.536000Z",
     "iopub.status.busy": "2024-05-18T10:29:19.535213Z",
     "iopub.status.idle": "2024-05-18T10:30:05.974289Z",
     "shell.execute_reply": "2024-05-18T10:30:05.973134Z"
    },
    "papermill": {
     "duration": 46.452786,
     "end_time": "2024-05-18T10:30:05.977322",
     "exception": false,
     "start_time": "2024-05-18T10:29:19.524536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\r\n",
      "  Downloading langchain-0.2.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\r\n",
      "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\r\n",
      "  Downloading langchain_core-0.2.0-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\r\n",
      "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\r\n",
      "  Downloading langsmith-0.1.59-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\r\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\r\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\r\n",
      "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\r\n",
      "Downloading langchain-0.2.0-py3-none-any.whl (973 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_core-0.2.0-py3-none-any.whl (307 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\r\n",
      "Downloading langsmith-0.1.59-py3-none-any.whl (121 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.2/121.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: packaging, orjson, langsmith, langchain-core, langchain-text-splitters, langchain\r\n",
      "  Attempting uninstall: packaging\r\n",
      "    Found existing installation: packaging 21.3\r\n",
      "    Uninstalling packaging-21.3:\r\n",
      "      Successfully uninstalled packaging-21.3\r\n",
      "  Attempting uninstall: orjson\r\n",
      "    Found existing installation: orjson 3.9.10\r\n",
      "    Uninstalling orjson-3.9.10:\r\n",
      "      Successfully uninstalled orjson-3.9.10\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "keras-cv 0.8.2 requires keras-core, which is not installed.\r\n",
      "keras-nlp 0.9.3 requires keras-core, which is not installed.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\r\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\r\n",
      "jupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "osmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\r\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed langchain-0.2.0 langchain-core-0.2.0 langchain-text-splitters-0.2.0 langsmith-0.1.59 orjson-3.10.3 packaging-23.2\r\n",
      "Collecting langchain_community\r\n",
      "  Downloading langchain_community-0.2.0-py3-none-any.whl.metadata (8.8 kB)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (6.0.1)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.0.25)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (3.9.1)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.6.4)\r\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.2.0)\r\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.2.0)\r\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.1.59)\r\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (1.26.4)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.31.0)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (8.2.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.1)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\r\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.0)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.5.3)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\r\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2024.2.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.9.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (2.4)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.14.6)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\r\n",
      "Downloading langchain_community-0.2.0-py3-none-any.whl (2.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: langchain_community\r\n",
      "Successfully installed langchain_community-0.2.0\r\n",
      "Collecting openai\r\n",
      "  Downloading openai-1.30.1-py3-none-any.whl.metadata (21 kB)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.2.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.5.3)\r\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.0)\r\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.1)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from openai) (4.9.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\r\n",
      "Downloading openai-1.30.1-py3-none-any.whl (320 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: openai\r\n",
      "Successfully installed openai-1.30.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain_community\n",
    "!pip install openai\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "openai.api_key = user_secrets.get_secret(\"openai_api\")\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98795d3b",
   "metadata": {
    "papermill": {
     "duration": 0.01311,
     "end_time": "2024-05-18T10:30:06.004595",
     "exception": false,
     "start_time": "2024-05-18T10:30:05.991485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"2\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 2. Conversation Buffer Memory </b></div>\n",
    "\n",
    "\n",
    "Let’s start with a motivating example for memory, using LangChain to manage a chat or a chatbot conversation. I am going to set the LLM as a chat interface of OpenAI with a temperature equal to 0. We will use the memory as a ConversationBufferMemory and then build a conversation chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44312c3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:06.034288Z",
     "iopub.status.busy": "2024-05-18T10:30:06.033796Z",
     "iopub.status.idle": "2024-05-18T10:30:06.705296Z",
     "shell.execute_reply": "2024-05-18T10:30:06.704239Z"
    },
    "papermill": {
     "duration": 0.689924,
     "end_time": "2024-05-18T10:30:06.707779",
     "exception": false,
     "start_time": "2024-05-18T10:30:06.017855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model, openai_api_key=openai.api_key)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a7b567",
   "metadata": {
    "papermill": {
     "duration": 0.013909,
     "end_time": "2024-05-18T10:30:06.734869",
     "exception": false,
     "start_time": "2024-05-18T10:30:06.720960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's start a conversation using conversation.predict() and the given input will be “Hi, my name is Youssef” and let’s see the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca211a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:06.763309Z",
     "iopub.status.busy": "2024-05-18T10:30:06.762704Z",
     "iopub.status.idle": "2024-05-18T10:30:07.640045Z",
     "shell.execute_reply": "2024-05-18T10:30:07.638579Z"
    },
    "papermill": {
     "duration": 0.894683,
     "end_time": "2024-05-18T10:30:07.642831",
     "exception": false,
     "start_time": "2024-05-18T10:30:06.748148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Youssef\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Youssef! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Youssef\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac939b1b",
   "metadata": {
    "papermill": {
     "duration": 0.013488,
     "end_time": "2024-05-18T10:30:07.669578",
     "exception": false,
     "start_time": "2024-05-18T10:30:07.656090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Then, let’s ask it, what is 1 plus 1? 1 plus 1 is 2, and then ask it again, you know, what’s my name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6f793d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:07.700068Z",
     "iopub.status.busy": "2024-05-18T10:30:07.699703Z",
     "iopub.status.idle": "2024-05-18T10:30:08.568065Z",
     "shell.execute_reply": "2024-05-18T10:30:08.566801Z"
    },
    "papermill": {
     "duration": 0.887678,
     "end_time": "2024-05-18T10:30:08.570586",
     "exception": false,
     "start_time": "2024-05-18T10:30:07.682908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Youssef\n",
      "AI: Hello Youssef! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03b94d",
   "metadata": {
    "papermill": {
     "duration": 0.012982,
     "end_time": "2024-05-18T10:30:08.597002",
     "exception": false,
     "start_time": "2024-05-18T10:30:08.584020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we will ask it again, “What’s my name?” and you can see its response “Your name is Youssef, as you mentioned earlier”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22dc568c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:08.625372Z",
     "iopub.status.busy": "2024-05-18T10:30:08.625023Z",
     "iopub.status.idle": "2024-05-18T10:30:09.314279Z",
     "shell.execute_reply": "2024-05-18T10:30:09.313089Z"
    },
    "papermill": {
     "duration": 0.709563,
     "end_time": "2024-05-18T10:30:09.319843",
     "exception": false,
     "start_time": "2024-05-18T10:30:08.610280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Youssef\n",
      "AI: Hello Youssef! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Youssef. Is there anything else you would like to know or discuss?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b0a07",
   "metadata": {
    "papermill": {
     "duration": 0.013327,
     "end_time": "2024-05-18T10:30:09.347237",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.333910",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So this is a prompt that LangChain has generated to have the system have a hopeful and friendly conversation with you. When you execute this on the second and third parts of the conversations, it keeps the prompt as follows. \n",
    "\n",
    "Since I have used the memory variable to store the memory. So if I were to print memory.buffer, it has stored the conversation so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4821c94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:09.376780Z",
     "iopub.status.busy": "2024-05-18T10:30:09.376098Z",
     "iopub.status.idle": "2024-05-18T10:30:09.381342Z",
     "shell.execute_reply": "2024-05-18T10:30:09.380305Z"
    },
    "papermill": {
     "duration": 0.023273,
     "end_time": "2024-05-18T10:30:09.384045",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.360772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Youssef\n",
      "AI: Hello Youssef! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is Youssef. Is there anything else you would like to know or discuss?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6c78a",
   "metadata": {
    "papermill": {
     "duration": 0.013384,
     "end_time": "2024-05-18T10:30:09.411129",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.397745",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can also print this out using the memory.loadMemoryVariables({}). The curly braces here are an empty dictionary. There are some more advanced features that you can use with a more sophisticated input, but it is not covered in this article but you can find it in LangChain documentation. So don’t worry about why there’s an empty curly braces here. But this is what LangChain has remembered in the memory of the conversation so far. It’s just everything that the AI or the human has said."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1a7dbe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:09.440668Z",
     "iopub.status.busy": "2024-05-18T10:30:09.440299Z",
     "iopub.status.idle": "2024-05-18T10:30:09.446389Z",
     "shell.execute_reply": "2024-05-18T10:30:09.445334Z"
    },
    "papermill": {
     "duration": 0.023636,
     "end_time": "2024-05-18T10:30:09.448655",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.425019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Youssef\\nAI: Hello Youssef! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1 + 1 equals 2. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Youssef. Is there anything else you would like to know or discuss?\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b4c9d",
   "metadata": {
    "papermill": {
     "duration": 0.01337,
     "end_time": "2024-05-18T10:30:09.475760",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.462390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The way that LangChain is storing the conversation is with this ConversationBufferMemory. If I were to use the ConversationBufferMemory to specify a couple of inputs and outputs, this is how you add new things to the memory if you wish to do so explicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d2ed00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:09.508250Z",
     "iopub.status.busy": "2024-05-18T10:30:09.507832Z",
     "iopub.status.idle": "2024-05-18T10:30:09.513602Z",
     "shell.execute_reply": "2024-05-18T10:30:09.512303Z"
    },
    "papermill": {
     "duration": 0.025316,
     "end_time": "2024-05-18T10:30:09.515997",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.490681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7abc499",
   "metadata": {
    "papermill": {
     "duration": 0.014246,
     "end_time": "2024-05-18T10:30:09.546149",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.531903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Memory.saveContext says, hi, what’s up? I know this is not the most exciting conversation, but I wanted to give a short example. And with that, this is what the status of the memory is. Once again, let me show the memory variables. Now, if you want to add additional data to the memory, you can keep on saving additional context. So, the conversation goes on, not much, just hanging, cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f53a3773",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:09.576794Z",
     "iopub.status.busy": "2024-05-18T10:30:09.576429Z",
     "iopub.status.idle": "2024-05-18T10:30:09.581194Z",
     "shell.execute_reply": "2024-05-18T10:30:09.580135Z"
    },
    "papermill": {
     "duration": 0.022504,
     "end_time": "2024-05-18T10:30:09.583165",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.560661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f8f10",
   "metadata": {
    "papermill": {
     "duration": 0.013506,
     "end_time": "2024-05-18T10:30:09.611052",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.597546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When you use a large language model for a chat conversation, the large language model itself is stateless. The language model itself does not remember the conversation you’ve had so far. Therefore each call to the API endpoint is independent. Chatbots have memory only because there’s usually rapid code that provides the full conversation that’s been had so far as context to the LLM. Therefore the memory can store explicitly the terms or the utterances so far. Hi, my name is Youssef. Hello, it’s just nice to meet you and so on. \n",
    "\n",
    "This memory storage is used as input or additional context to the LLM so that it can generate output as if it’s just having the next conversational turn, knowing what’s been said before. \n",
    "\n",
    "As the conversation becomes long, the amount of memory needed becomes long, and the cost of sending a lot of tokens to the LLM, which usually charges based on the number of tokens it needs to process, will also become more expensive. \n",
    "\n",
    "So LangChain provides several convenient kinds of memory to store and accumulate the conversation. So far, we’ve been looking at the ConversationBufferMemory. Let’s look at a different type of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a6ee0b",
   "metadata": {
    "papermill": {
     "duration": 0.014578,
     "end_time": "2024-05-18T10:30:09.639694",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.625116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"3\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 3. Conversation Buffer Window Memory</b></div>\n",
    "\n",
    "Let's start with importing the conversation buffer window memory that only keeps a window of memory. If I set memory to conversational buffer window memory with k equals one, the variable k =1 specifies that I want to remember just one conversational exchange. That is one utterance from me and one utterance from a chatbot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6a33c8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:09.734977Z",
     "iopub.status.busy": "2024-05-18T10:30:09.734579Z",
     "iopub.status.idle": "2024-05-18T10:30:09.741986Z",
     "shell.execute_reply": "2024-05-18T10:30:09.740750Z"
    },
    "papermill": {
     "duration": 0.089918,
     "end_time": "2024-05-18T10:30:09.744244",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.654326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.load_memory_variables({}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f27f91c",
   "metadata": {
    "papermill": {
     "duration": 0.015002,
     "end_time": "2024-05-18T10:30:09.773590",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.758588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If I were to have it save context, “Hi, what’s up, not much, just hanging”. If I were to look at memory.load_memory_variables({}), it only remembers the most recent utterance as you can see below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18ebaf61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:09.804075Z",
     "iopub.status.busy": "2024-05-18T10:30:09.803675Z",
     "iopub.status.idle": "2024-05-18T10:30:09.811046Z",
     "shell.execute_reply": "2024-05-18T10:30:09.809723Z"
    },
    "papermill": {
     "duration": 0.025356,
     "end_time": "2024-05-18T10:30:09.813265",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.787909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d278347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T08:06:31.603370Z",
     "iopub.status.busy": "2024-05-18T08:06:31.602687Z",
     "iopub.status.idle": "2024-05-18T08:06:31.613020Z",
     "shell.execute_reply": "2024-05-18T08:06:31.611658Z",
     "shell.execute_reply.started": "2024-05-18T08:06:31.603334Z"
    },
    "papermill": {
     "duration": 0.013866,
     "end_time": "2024-05-18T10:30:09.841382",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.827516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It only returns the recent conversation because k was equal to one. So this is a nice feature because it lets you keep track of just the most recent few conversational terms. In practice, you probably won’t use this with k equals one. You use this with k set to a larger number.\n",
    "\n",
    " If I were to rerun the conversation that we had above, we would say “Hi, my name is Youssef”. Then we will say “What is 1 plus 1?”. Finally, I will ask it “What is my name?”. Because we set k equals 1, it only remembers the last exchange versus what is 1 plus 1? The answer is 1 plus 1 equals 2, and it’s forgotten this early exchange which is now, now says, sorry, don’t have access to that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "082685dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:09.871520Z",
     "iopub.status.busy": "2024-05-18T10:30:09.871113Z",
     "iopub.status.idle": "2024-05-18T10:30:10.699837Z",
     "shell.execute_reply": "2024-05-18T10:30:10.698792Z"
    },
    "papermill": {
     "duration": 0.846527,
     "end_time": "2024-05-18T10:30:10.702345",
     "exception": false,
     "start_time": "2024-05-18T10:30:09.855818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Youssef\n",
      "AI: Hello Youssef! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is Youssef. Is there anything else you would like to know or discuss?\n",
      "Human: Hi, my name is Youssef\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Youssef! It seems like you've already introduced yourself, but it's nice to hear it again. How can I assist you today?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Youssef\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0343ea64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:10.733119Z",
     "iopub.status.busy": "2024-05-18T10:30:10.732717Z",
     "iopub.status.idle": "2024-05-18T10:30:11.934410Z",
     "shell.execute_reply": "2024-05-18T10:30:11.933635Z"
    },
    "papermill": {
     "duration": 1.219704,
     "end_time": "2024-05-18T10:30:11.936525",
     "exception": false,
     "start_time": "2024-05-18T10:30:10.716821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Youssef\n",
      "AI: Hello Youssef! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is Youssef. Is there anything else you would like to know or discuss?\n",
      "Human: Hi, my name is Youssef\n",
      "AI: Hello Youssef! It seems like you've already introduced yourself, but it's nice to hear it again. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2. Is there anything else you would like to know or discuss, Youssef?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34cc6101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:11.969769Z",
     "iopub.status.busy": "2024-05-18T10:30:11.969395Z",
     "iopub.status.idle": "2024-05-18T10:30:12.766994Z",
     "shell.execute_reply": "2024-05-18T10:30:12.765794Z"
    },
    "papermill": {
     "duration": 0.816977,
     "end_time": "2024-05-18T10:30:12.769013",
     "exception": false,
     "start_time": "2024-05-18T10:30:11.952036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Youssef\n",
      "AI: Hello Youssef! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is Youssef. Is there anything else you would like to know or discuss?\n",
      "Human: Hi, my name is Youssef\n",
      "AI: Hello Youssef! It seems like you've already introduced yourself, but it's nice to hear it again. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know or discuss, Youssef?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Youssef. Is there anything else you would like to know or discuss?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce3b11",
   "metadata": {
    "papermill": {
     "duration": 0.016959,
     "end_time": "2024-05-18T10:30:12.801639",
     "exception": false,
     "start_time": "2024-05-18T10:30:12.784680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 4. Conversation Token Buffer Memory</b></div>\n",
    "\n",
    "With the conversational token buffer memory, the memory will limit the number of tokens saved. Since a lot of LLM pricing is based on tokens, this maps more directly to the cost of the LLM calls. Let's set the max token limit to 50. Let’s input the following conversation “AI is what? Amazing. Backpropagation is what? Beautiful. Chatbot is what? Charming”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0672851",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:12.835304Z",
     "iopub.status.busy": "2024-05-18T10:30:12.834222Z",
     "iopub.status.idle": "2024-05-18T10:30:27.196306Z",
     "shell.execute_reply": "2024-05-18T10:30:27.194781Z"
    },
    "papermill": {
     "duration": 14.38133,
     "end_time": "2024-05-18T10:30:27.198904",
     "exception": false,
     "start_time": "2024-05-18T10:30:12.817574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\r\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\r\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tiktoken\r\n",
      "Successfully installed tiktoken-0.7.0\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "\n",
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ade7d",
   "metadata": {
    "papermill": {
     "duration": 0.017299,
     "end_time": "2024-05-18T10:30:27.232925",
     "exception": false,
     "start_time": "2024-05-18T10:30:27.215626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " If I run this with a high token limit, it has almost the whole conversation. If I increase the token limit to 100, it now has the whole conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6453583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:27.268039Z",
     "iopub.status.busy": "2024-05-18T10:30:27.267665Z",
     "iopub.status.idle": "2024-05-18T10:30:27.279269Z",
     "shell.execute_reply": "2024-05-18T10:30:27.277658Z"
    },
    "papermill": {
     "duration": 0.032064,
     "end_time": "2024-05-18T10:30:27.282056",
     "exception": false,
     "start_time": "2024-05-18T10:30:27.249992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0ec1f",
   "metadata": {
    "papermill": {
     "duration": 0.016053,
     "end_time": "2024-05-18T10:30:27.315385",
     "exception": false,
     "start_time": "2024-05-18T10:30:27.299332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If I decrease it to 20, Then, you know, it chops off the earlier parts of this conversation to retain the number of tokens corresponding to the most recent exchanges but subject to not exceeding the token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06c97185",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:27.349141Z",
     "iopub.status.busy": "2024-05-18T10:30:27.348760Z",
     "iopub.status.idle": "2024-05-18T10:30:27.359024Z",
     "shell.execute_reply": "2024-05-18T10:30:27.357948Z"
    },
    "papermill": {
     "duration": 0.030113,
     "end_time": "2024-05-18T10:30:27.361384",
     "exception": false,
     "start_time": "2024-05-18T10:30:27.331271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Charming!'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=20)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261d59b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T08:16:31.814667Z",
     "iopub.status.busy": "2024-05-18T08:16:31.814248Z",
     "iopub.status.idle": "2024-05-18T08:16:31.821513Z",
     "shell.execute_reply": "2024-05-18T08:16:31.820015Z",
     "shell.execute_reply.started": "2024-05-18T08:16:31.814629Z"
    },
    "papermill": {
     "duration": 0.015805,
     "end_time": "2024-05-18T10:30:27.393053",
     "exception": false,
     "start_time": "2024-05-18T10:30:27.377248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"5\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 5. Conversation Summary Memory</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f97b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T08:16:45.634640Z",
     "iopub.status.busy": "2024-05-18T08:16:45.634188Z",
     "iopub.status.idle": "2024-05-18T08:16:45.643295Z",
     "shell.execute_reply": "2024-05-18T08:16:45.641861Z",
     "shell.execute_reply.started": "2024-05-18T08:16:45.634571Z"
    },
    "papermill": {
     "duration": 0.016021,
     "end_time": "2024-05-18T10:30:27.424985",
     "exception": false,
     "start_time": "2024-05-18T10:30:27.408964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, there’s one last type of memory we will explore, which is the conversation summary buffer memory. The idea is that instead of limiting the memory to a fixed number of tokens based on the most recent utterances or a fixed number of conversational exchanges, let’s use an LLM to write a summary of the conversation and let that be the memory.\n",
    "\n",
    "So here’s an example where I’m going to create a long string to schedule a meeting with the product team:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d442071e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:27.458361Z",
     "iopub.status.busy": "2024-05-18T10:30:27.457992Z",
     "iopub.status.idle": "2024-05-18T10:30:27.463524Z",
     "shell.execute_reply": "2024-05-18T10:30:27.462341Z"
    },
    "papermill": {
     "duration": 0.02498,
     "end_time": "2024-05-18T10:30:27.465849",
     "exception": false,
     "start_time": "2024-05-18T10:30:27.440869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab147d0",
   "metadata": {
    "papermill": {
     "duration": 0.015691,
     "end_time": "2024-05-18T10:30:27.497595",
     "exception": false,
     "start_time": "2024-05-18T10:30:27.481904",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will use a conversation summary buffer memory with a max token limit of 400 in this case which is a pretty high token limit, and I’m going to insert in a few conversational terms in which we start with “Hello, what’s up, Not much just hanging, cool, what is on the schedule today, and the response is the long schedule above”. \n",
    "\n",
    "So this memory now has quite a lot of text in it. Let’s take a look at the memory variables. It contains that entire piece of text because 400 tokens were sufficient to store all this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab6b3b8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:27.531370Z",
     "iopub.status.busy": "2024-05-18T10:30:27.530463Z",
     "iopub.status.idle": "2024-05-18T10:30:28.777118Z",
     "shell.execute_reply": "2024-05-18T10:30:28.775769Z"
    },
    "papermill": {
     "duration": 1.266382,
     "end_time": "2024-05-18T10:30:28.779834",
     "exception": false,
     "start_time": "2024-05-18T10:30:27.513452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human and AI exchange greetings and discuss the day's schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the power of LangChain as a tool.\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2d401",
   "metadata": {
    "papermill": {
     "duration": 0.016227,
     "end_time": "2024-05-18T10:30:28.813898",
     "exception": false,
     "start_time": "2024-05-18T10:30:28.797671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If we were to reduce the max token limit to 100 tokens, then the conversation summary buffer memory would use an LLM, the OpenAI endpoint in this case to generate a summary of the conversation as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10ff5cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:28.848555Z",
     "iopub.status.busy": "2024-05-18T10:30:28.848160Z",
     "iopub.status.idle": "2024-05-18T10:30:28.854412Z",
     "shell.execute_reply": "2024-05-18T10:30:28.853614Z"
    },
    "papermill": {
     "duration": 0.026158,
     "end_time": "2024-05-18T10:30:28.856503",
     "exception": false,
     "start_time": "2024-05-18T10:30:28.830345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=400)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0e37d",
   "metadata": {
    "papermill": {
     "duration": 0.015752,
     "end_time": "2024-05-18T10:30:28.888353",
     "exception": false,
     "start_time": "2024-05-18T10:30:28.872601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If we were to have a conversation, using this LLM, let me create a conversation chain, the same as before. Let’s say that we were to give an input “What would be a good demo to show” and I would set the verbose equals true. The LLM thinks the current conversation has had this discussion so far because that’s the summary of the conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de8f34ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T10:30:28.925978Z",
     "iopub.status.busy": "2024-05-18T10:30:28.925322Z",
     "iopub.status.idle": "2024-05-18T10:30:30.170026Z",
     "shell.execute_reply": "2024-05-18T10:30:30.168829Z"
    },
    "papermill": {
     "duration": 1.2659,
     "end_time": "2024-05-18T10:30:30.173257",
     "exception": false,
     "start_time": "2024-05-18T10:30:28.907357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: What is on the schedule today?\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I would recommend showcasing the Language Model for Language Chains (LLM) demo. It's a cutting-edge tool that uses advanced AI technology to generate natural language text based on a given prompt. It's sure to impress your customer and demonstrate the capabilities of our AI technology.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "conversation.predict(input=\"What would be a good demo to show?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d4851",
   "metadata": {
    "papermill": {
     "duration": 0.016065,
     "end_time": "2024-05-18T10:30:30.205758",
     "exception": false,
     "start_time": "2024-05-18T10:30:30.189693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With the conversation summary buffer memory, the aim is to explicitly store messages up to a specified token limit. In this case, we’re capping explicit storage at 100 tokens, as requested. Any additional content beyond this limit is summarized using CLM, as demonstrated above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b8b35",
   "metadata": {
    "papermill": {
     "duration": 0.016396,
     "end_time": "2024-05-18T10:30:30.238681",
     "exception": false,
     "start_time": "2024-05-18T10:30:30.222285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a0a861",
   "metadata": {
    "papermill": {
     "duration": 0.016089,
     "end_time": "2024-05-18T10:30:30.271045",
     "exception": false,
     "start_time": "2024-05-18T10:30:30.254956",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "LangChain also encompasses additional memory types, with one of the most notable being vector data memory. This feature proves particularly potent for those familiar with word embeddings and text embeddings, as it involves storing such embeddings within the vector database. Utilizing this type of vector database memory enables LangChain to retrieve the most relevant blocks of text, enhancing its memory capabilities significantly.\n",
    "\n",
    "When developing applications with LangChain, you have the flexibility to utilize various memory types. This includes leveraging conversation memory, as demonstrated in this article, along with entity memory to recall specific individuals. \n",
    "\n",
    "This approach allows for the retention of both a summarized version of the conversation and explicit details about key individuals involved. Additionally, developers often opt to store the entire conversation in a conventional database, such as a key-value store or SQL database.\n",
    "\n",
    " This enables easy reference back to the conversation for auditing purposes or further system enhancements. So, these memory types offer a robust framework for building your applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289bd3ec",
   "metadata": {
    "papermill": {
     "duration": 0.015943,
     "end_time": "2024-05-18T10:30:30.303469",
     "exception": false,
     "start_time": "2024-05-18T10:30:30.287526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"box-shadow: rgba(240, 46, 170, 0.4) -5px 5px inset, rgba(240, 46, 170, 0.3) -10px 10px inset, rgba(240, 46, 170, 0.2) -15px 15px inset, rgba(240, 46, 170, 0.1) -20px 20px inset, rgba(240, 46, 170, 0.05) -25px 25px inset; padding:20px; font-size:30px; font-family: consolas; display:fill; border-radius:15px; color: rgba(240, 46, 170, 0.7)\"> <b> ༼⁠ ⁠つ⁠ ⁠◕⁠‿⁠◕⁠ ⁠༽⁠つ Thank You!</b></div>\n",
    "\n",
    "<p style=\"font-family:verdana; color:rgb(34, 34, 34); font-family: consolas; font-size: 16px;\"> 💌 Thank you for taking the time to read through my notebook. I hope you found it interesting and informative. If you have any feedback or suggestions for improvement, please don't hesitate to let me know in the comments. <br><br> 🚀 If you liked this notebook, please consider upvoting it so that others can discover it too. Your support means a lot to me, and it helps to motivate me to create more content in the future. <br><br> ❤️ Once again, thank you for your support, and I hope to see you again soon!</p>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 74.357319,
   "end_time": "2024-05-18T10:30:31.041117",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-18T10:29:16.683798",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
